# 16 â€” Agent Memory (ReasoningBank) & Evolving Orchestration

> **Má»¥c Ä‘Ã­ch training:** Hiá»ƒu ReasoningBank â€” framework memory cho AI agents há»c tá»« kinh nghiá»‡m, vÃ  Evolving Orchestration â€” puppeteer paradigm cho multi-agent collaboration, káº¿t há»£p reinforcement learning.

---

## 1. ReasoningBank â€” Memory Framework cho AI Agents

### 1.1 Váº¥n Ä‘á»
```
HIá»†N Táº I â€” AGENTS KHÃ”NG CÃ“ MEMORY:
â”œâ”€ Má»—i task xá»­ lÃ½ Ä‘á»™c láº­p (isolated)
â”œâ”€ Láº·p láº¡i sai láº§m cÅ©
â”œâ”€ Bá» phÃ­ insights tá»« tasks trÆ°á»›c
â”œâ”€ KhÃ´ng phÃ¡t triá»ƒn skills theo thá»i gian
â””â”€ CÃ¡c memory cÅ© chá»‰ lÃ  "passive record-keeping"

GIáº¢I PHÃP â€” REASONINGBANK:
â”œâ”€ Distill strategies tá»« cáº£ SUCCESS & FAILURE
â”œâ”€ Structured memory items (khÃ´ng pháº£i raw logs)
â”œâ”€ Embedding-based retrieval khi gáº·p task má»›i
â”œâ”€ Continuous learning loop
â””â”€ "Actionable, generalizable guidance for future decisions"
```

- **TÃ¡c giáº£:** University of Illinois Urbana-Champaign + Google Cloud AI Research
- **Paper:** [ReasoningBank](https://arxiv.org/abs/2509.25140)
- **Nguá»“n:** VentureBeat â€” "New memory framework builds AI agents that can handle the real world's unpredictability"

### 1.2 CÃ¡ch hoáº¡t Ä‘á»™ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              REASONINGBANK LOOP                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  1. AGENT FACES NEW TASK                        â”‚
â”‚     â””â”€ Embedding search â†’ retrieve memories     â”‚
â”‚     â””â”€ Insert memories into system prompt        â”‚
â”‚        â”‚                                        â”‚
â”‚  2. AGENT EXECUTES TASK                         â”‚
â”‚     â””â”€ Uses memory hints to guide actions       â”‚
â”‚     â””â”€ Makes decisions with past context        â”‚
â”‚        â”‚                                        â”‚
â”‚  3. EVALUATE OUTCOME                            â”‚
â”‚     â””â”€ LLM-as-judge: success or failure?        â”‚
â”‚     â””â”€ No human labeling needed                 â”‚
â”‚        â”‚                                        â”‚
â”‚  4. EXTRACT INSIGHTS                            â”‚
â”‚     â””â”€ FROM SUCCESS: distill strategies         â”‚
â”‚     â””â”€ FROM FAILURE: distill preventive lessons â”‚
â”‚        â”‚                                        â”‚
â”‚  5. MERGE INTO REASONINGBANK                    â”‚
â”‚     â””â”€ Analyze + consolidate new memories       â”‚
â”‚     â””â”€ Remove redundant items                   â”‚
â”‚     â””â”€ Update existing strategies               â”‚
â”‚        â”‚                                        â”‚
â”‚  â†’ REPEAT (virtuous cycle)                      â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 VÃ­ dá»¥ thá»±c táº¿

```
TASK: "TÃ¬m tai nghe Sony trÃªn web"

AGENT KHÃ”NG CÃ“ MEMORY:
â”œâ”€ Search broad query â†’ 4000+ results irrelevant
â”œâ”€ Trial-and-error 8 steps â†’ váº«n chÆ°a Ä‘Ãºng
â”œâ”€ Tá»‘n 8x token costs
â””â”€ User experience kÃ©m

AGENT Vá»šI REASONINGBANK:
â”œâ”€ Retrieve memory: "optimize search query" + "use category filter"
â”œâ”€ Search "Sony headphones" + filter Electronics
â”œâ”€ 2 steps â†’ Ä‘Ãºng ngay
â”œâ”€ Tiáº¿t kiá»‡m ~2x operational costs
â””â”€ User experience tá»‘t hÆ¡n
```

### 1.4 Memory-aware Test-Time Scaling (MaTTS)

```
STANDARD TEST-TIME SCALING:
â”œâ”€ Generate multiple answers
â”œâ”€ Pick best one
â””â”€ Independent attempts (no learning)

MaTTS â€” MEMORY + SCALING:
â”œâ”€ PARALLEL: Generate N trajectories â†’ compare â†’ find patterns
â”œâ”€ SEQUENTIAL: Iteratively refine within single attempt
â”œâ”€ Memory guides toward promising solutions
â”œâ”€ Diverse experiences â†’ higher-quality memories
â””â”€ POSITIVE FEEDBACK LOOP: better memory â†’ better scaling â†’ better memory
```

### 1.5 Benchmark Results

| Benchmark | Improvement | Domain |
|-----------|------------|--------|
| **WebArena** | +8.3% success rate | Web browsing |
| **SWE-Bench-Verified** | Consistent improvement | Software engineering |
| **Cross-domain tasks** | Best generalization | Multi-domain |
| **Steps needed** | Reduced significantly | Efficiency |

**Tested with:** Gemini 2.5 Pro, Claude 3.7 Sonnet

---

## 2. Evolving Orchestration â€” Puppeteer Paradigm

### 2.1 Paper Overview
- **TÃªn:** Multi-Agent Collaboration via Evolving Orchestration
- **Venue:** Accepted at **NeurIPS 2025**
- **TÃ¡c giáº£:** Yufan Dang, Chen Qian et al. (OpenBMB / ChatDev team)
- **Link:** [arxiv:2505.19591](https://arxiv.org/abs/2505.19591)
- **Code:** [github.com/OpenBMB/ChatDev/tree/puppeteer](https://github.com/OpenBMB/ChatDev/tree/puppeteer)

### 2.2 Ã tÆ°á»Ÿng cá»‘t lÃµi

```
Váº¤N Äá»€ Vá»šI MULTI-AGENT HIá»†N Táº I:
â”œâ”€ Static organizational structures
â”œâ”€ KhÃ´ng adapt khi task complexity tÄƒng
â”œâ”€ Coordination overhead tÄƒng theo sá»‘ agents
â”œâ”€ Inefficiency khi agent numbers grow
â””â”€ Pre-defined workflows = rigid

GIáº¢I PHÃP â€” PUPPETEER PARADIGM:
â”œâ”€ Centralized orchestrator ("puppeteer")
â”œâ”€ Agents = "puppets" Ä‘Æ°á»£c direct dynamically
â”œâ”€ Orchestrator trained via Reinforcement Learning
â”œâ”€ Adaptive sequencing & prioritizing agents
â””â”€ Flexible, evolvable collective reasoning
```

### 2.3 Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PUPPETEER PARADIGM                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â”‚ ORCHESTRATOR â”‚ â† RL-trained      â”‚
â”‚              â”‚ (Puppeteer)  â”‚                   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                     â”‚                           â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚          â”‚          â”‚          â”‚                 â”‚
â”‚     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”          â”‚
â”‚     â”‚Agent A â”‚ â”‚Agent B â”‚ â”‚Agent C â”‚          â”‚
â”‚     â”‚(Puppet)â”‚ â”‚(Puppet)â”‚ â”‚(Puppet)â”‚          â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                 â”‚
â”‚  KEY INSIGHT:                                   â”‚
â”‚  Orchestrator learns WHEN and HOW to             â”‚
â”‚  sequence agents through RL training,            â”‚
â”‚  discovering CYCLIC reasoning structures.        â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.4 Key Findings

| Finding | Detail |
|---------|--------|
| **Superior performance** | Outperforms static multi-agent structures |
| **Reduced computational costs** | More efficient agent sequencing |
| **Cyclic reasoning** | Emerges naturally â€” agents revisit previous conclusions |
| **Evolvable** | Orchestrator improves over time via RL |
| **Scalable** | Handles growing task complexity + agent numbers |

### 2.5 Cyclic Reasoning Structures

```
STATIC APPROACH (Linear):
  A â†’ B â†’ C â†’ Output

EVOLVED APPROACH (Cyclic):
  A â†’ B â†’ C â†’ A â†’ B â†’ Output
  
  WHY? Some problems need agents to REVISIT
  and REFINE earlier conclusions based on 
  new information from later agents.
  
  The orchestrator LEARNS this pattern via RL,
  discovering that cycling back improves quality.
```

---

## 3. Ãp dá»¥ng cho EMADS-PR

### 3.1 ReasoningBank cho má»—i Agent Role

```python
# Concept: Memory bank per agent role
reasoning_banks = {
    "CTO": ReasoningBank(
        strategies=["architecture patterns", "tech debt assessment"],
        failure_lessons=["over-engineering risks", "scaling bottlenecks"]
    ),
    "COO": ReasoningBank(
        strategies=["resource allocation", "timeline estimation"],
        failure_lessons=["understaffing patterns", "vendor risks"]
    ),
    "ReconcileGPT": ReasoningBank(
        strategies=["conflict resolution patterns", "trade-off analysis"],
        failure_lessons=["false consensus detection"]
    )
}

# When new task arrives:
def process_with_memory(agent_role, task):
    bank = reasoning_banks[agent_role]
    
    # 1. Retrieve relevant memories
    memories = bank.retrieve(task, top_k=5)
    
    # 2. Inject into system prompt
    enhanced_prompt = f"""
    {agent_system_prompt}
    
    PAST EXPERIENCE (apply if relevant):
    {format_memories(memories)}
    """
    
    # 3. Execute task
    result = agent.invoke(enhanced_prompt, task)
    
    # 4. Extract new insights
    insights = bank.extract_insights(task, result)
    bank.merge(insights)
    
    return result
```

### 3.2 Evolving Orchestration cho EMADS-PR

```python
# Current EMADS-PR: Static flow
# CEO â†’ Orchestrator â†’ [CTO + COO + Legal + Risk + Cost] â†’ Reconcile â†’ Human

# Evolved EMADS-PR: RL-trained orchestrator
class EvolvedOrchestrator:
    def __init__(self):
        self.policy = RLPolicy()  # Trained via RL
    
    def route(self, task, agent_outputs, iteration):
        """Dynamically decide next agent(s) to invoke"""
        state = {
            "task": task,
            "outputs_so_far": agent_outputs,
            "iteration": iteration,
            "complexity": self.assess_complexity(task)
        }
        
        # RL policy decides:
        # - Which agents to invoke next
        # - Whether to cycle back to previous agents
        # - When to finalize and send to ReconcileGPT
        action = self.policy.decide(state)
        
        return action  # e.g., {"invoke": ["CTO", "COO"], "cycle_back": False}
```

### 3.3 Shared Memory across Sessions

```
SESSION 1: "Migrate database?"
â”œâ”€ CTO recommends PostgreSQL â†’ approved â†’ success
â””â”€ Memory stored: "PostgreSQL good for structured data, team familiar"

SESSION 2: "New microservice needs database"  
â”œâ”€ CTO retrieves memory â†’ immediately suggests PostgreSQL
â”œâ”€ Adds: "Consider read replicas for high-traffic"
â””â”€ Faster decision, consistent with previous choices

SESSION 3: "Performance issues with PostgreSQL"
â”œâ”€ CTO retrieves both success + failure memories
â”œâ”€ Learns: "Add connection pooling when >100 concurrent"
â””â”€ More nuanced recommendations over time
```

---

## 4. Compositional Intelligence â€” TÆ°Æ¡ng lai

```
HIá»†N Táº I: Má»—i task = isolated
TÆ¯Æ NG LAI: Agent builds modular skills over time

VÃ­ dá»¥ cho Coding Agent:
â”œâ”€ Task 1: API integration â†’ learn skill "API patterns"
â”œâ”€ Task 2: Database management â†’ learn skill "DB operations"
â”œâ”€ Task 3: Both API + DB needed â†’ RECOMBINE learned skills
â”œâ”€ Task 4: + Authentication â†’ ADD new skill, RECOMBINE 3 skills
â””â”€ â†’ Agent autonomously assembles knowledge for complex workflows

"Over time, these modular skills become building blocks 
the agent can flexibly recombine to solve more complex tasks"
â€” Jun Yan, Google Research
```

---

## 5. Key Takeaways cho Agent

```
âœ… ReasoningBank: memory framework há»c tá»« cáº£ success & failure
âœ… LLM-as-judge: tá»± Ä‘Ã¡nh giÃ¡ outcome, khÃ´ng cáº§n human labeling
âœ… MaTTS: káº¿t há»£p memory + test-time scaling = positive feedback loop
âœ… Puppeteer paradigm: RL-trained orchestrator > static workflows
âœ… Cyclic reasoning: agents revisit conclusions â†’ better quality
âœ… Shared memory: agents improve across sessions
âœ… Compositional intelligence: modular skills â†’ recombine for new tasks
âœ… EMADS-PR upgrade path: ReasoningBank per agent + RL orchestrator
```

---

## ðŸ“š Sources

- VentureBeat: [ReasoningBank Memory Framework](https://venturebeat.com/ai/new-memory-framework-builds-ai-agents-that-can-handle-the-real-worlds)
- arXiv: [ReasoningBank Paper](https://arxiv.org/abs/2509.25140)
- arXiv: [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591) â€” NeurIPS 2025
- GitHub: [ChatDev Puppeteer](https://github.com/OpenBMB/ChatDev/tree/puppeteer)
